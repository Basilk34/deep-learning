import streamlit as st
st.set_page_config(page_title="ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙŠÙˆÙ„", page_icon="ğŸ’¬")

import tensorflow as tf
import numpy as np
import pickle
from tensorflow.keras.preprocessing.sequence import pad_sequences
from collections import Counter
import re
import datetime
import matplotlib.pyplot as plt

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
max_len = 100  # ØªØ£ÙƒØ¯ Ø£Ù†Ù‡Ø§ Ù†ÙØ³ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ØªÙŠ Ø¯Ø±Ø¨Øª Ø¨Ù‡Ø§
labels = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']

# --- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙˆÙƒÙ†ÙŠØ²Ø± ---
@st.cache_resource
def load_model_and_tokenizer():
    model = tf.keras.models.load_model('lstm_corona_model.h5')
    with open('tokenizer (1).pickle', 'rb') as f:
        tokenizer = pickle.load(f)
    return model, tokenizer

model, tokenizer = load_model_and_tokenizer()

# --- Ø¯Ø§Ù„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ---
def extract_keywords(text, num_keywords=5):
    words = re.findall(r'\b\w+\b', text.lower())
    stopwords = set(['the', 'is', 'in', 'and', 'to', 'of', 'a', 'for', 'on', 'with', 'that', 'this'])
    filtered_words = [w for w in words if w not in stopwords]
    most_common = Counter(filtered_words).most_common(num_keywords)
    return most_common  # ØªØ±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† tuples (ÙƒÙ„Ù…Ø©ØŒ ØªÙƒØ±Ø§Ø±)

# --- ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ---
st.title("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙŠÙˆÙ„ ØªØ¬Ø§Ù‡ ÙƒÙˆØ±ÙˆÙ†Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LSTM")

# Ø£ÙˆÙ„Ø§Ù‹: Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ù„ØªØ§Ø±ÙŠØ®
username = st.text_input("ğŸ‘¤ Ø£Ø¯Ø®Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:")
date = st.date_input("ğŸ“… Ø§Ø®ØªØ± Ø§Ù„ØªØ§Ø±ÙŠØ®:", datetime.date.today())

# ÙÙ‚Ø· Ø¥Ø°Ø§ ØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù†Ø¹Ø±Ø¶ Ø®Ø§Ù†Ø© Ø§Ù„Ù†Øµ ÙˆØ²Ø± Ø§Ù„ØªØ­Ù„ÙŠÙ„
if username:
    user_input = st.text_area("ğŸ“ Ø£Ø¯Ø®Ù„ ØªØºØ±ÙŠØ¯Ø© Ø£Ùˆ Ù†Øµ ØªØ­Ù„ÙŠÙ„:")
    if st.button("ØªØ­Ù„ÙŠÙ„"):
        if not user_input.strip():
            st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù„Ù„ØªØ­Ù„ÙŠÙ„!")
        else:
            # Ø§Ù„ØªÙ†Ø¨Ø¤
            seq = tokenizer.texts_to_sequences([user_input])
            padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')
            prediction = model.predict(padded)
            class_idx = np.argmax(prediction)
            confidence = np.max(prediction)

            # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            st.success(f"**Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** {username}")
            st.success(f"**Ø§Ù„ØªØ§Ø±ÙŠØ®:** {date}")
            st.success(f"**Ø§Ù„ØªØµÙ†ÙŠÙ:** {labels[class_idx]}")
            st.info(f"**Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©:** {confidence:.2f}")

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            keywords = extract_keywords(user_input)
            if keywords:
                st.markdown("### ğŸ”‘ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± Ø¸Ù‡ÙˆØ±Ù‹Ø§:")
                words, counts = zip(*keywords)
                st.write(", ".join(words))

                # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ
                fig, ax = plt.subplots()
                ax.bar(words, counts, color='skyblue')
                ax.set_ylabel('Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±')
                ax.set_title('Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©')
                st.pyplot(fig)
else:
    st.info("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„Ø¨Ø¯Ø¡.")

import streamlit as st
import os
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import numpy as np
import gdown
from PIL import Image

# Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
IMG_SIZE = 224

# Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø§Ø³ØªØ®Ø±Ø¬Øª Ø§Ù„Ù€ FILE_ID Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø­Ù‚Ùƒ)
MODEL_URL = 'https://drive.google.com/uc?export=download&id=1oYuyEpzubzQ2Ph67l3ZKvDjQYCcLwQjY'
MODEL_PATH = 'flower_model.h5'

@st.cache_resource
def download_and_load_model():
    if not os.path.exists(MODEL_PATH):
        st.write("â³ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Google Drive ...")
        gdown.download(MODEL_URL, MODEL_PATH, quiet=False)
    model = load_model(MODEL_PATH)
    return model

model = download_and_load_model()

def predict(img, model):
    img = img.resize((IMG_SIZE, IMG_SIZE))
    img_array = image.img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)
    prediction = model.predict(img_array)
    return prediction

st.title("ØªØµÙ†ÙŠÙ ØµÙˆØ± Ø§Ù„ÙˆØ±ÙˆØ¯")

uploaded_file = st.file_uploader("Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© ÙˆØ±Ø¯ Ù‡Ù†Ø§", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    img = Image.open(uploaded_file)
    st.image(img, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ØªÙŠ Ø±ÙØ¹ØªÙ‡Ø§", use_column_width=True)
    
    prediction = predict(img, model)
    
    # Ø¶Ø¹ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£ØµÙ†Ø§Ù Ø­Ø³Ø¨ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù†Ø¯Ùƒ
    class_labels = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']
    
    predicted_class = class_labels[np.argmax(prediction)]
    confidence = np.max(prediction) * 100
    
    st.write(f"**Ø§Ù„ØªÙ†Ø¨Ø¤:** {predicted_class}")
    st.write(f"**Ø§Ù„Ø«Ù‚Ø©:** {confidence:.2f}%")

