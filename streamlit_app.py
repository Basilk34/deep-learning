import streamlit as st
import snscrape.modules.twitter as sntwitter
import tensorflow as tf
import numpy as np
import pickle
from tensorflow.keras.preprocessing.sequence import pad_sequences
# ------------------------
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙˆÙƒÙ†ÙŠØ²Ø±
# ------------------------
model = tf.keras.models.load_model("text_model.h5")
with open("tokenizer.pkl", "rb") as f:
    tokenizer = pickle.load(f)

label_map = ["Ø³Ù„Ø¨ÙŠ", "Ù…Ø­Ø§ÙŠØ¯", "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"]

# ------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯ Streamlit
# ------------------------
st.set_page_config(page_title="ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ¬Ù‡", layout="wide")
st.title("ğŸ“Š ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ¬Ù‡ - Basil Kanaan")

# ------------------------
# ØªØ¨ÙˆÙŠØ¨Ø§Øª Streamlit
# ------------------------
tab1, tab2, tab3 = st.tabs([
    "ğŸ” ØªØ­Ù„ÙŠÙ„ Ù†Øµ Ø¨Ø­Ø«ÙŠ",
    "ğŸ¦ ØªØ­Ù„ÙŠÙ„ ØªØºØ±ÙŠØ¯Ø§Øª Twitter",
    "ğŸ“‚ Ø¨Ø­Ø« ÙÙŠ Ù…Ù†Ø´ÙˆØ±Ø§Øª ÙˆÙ‡Ù…ÙŠØ©"
])

# ------------------------
# Ø§Ù„ØªØ¨ÙˆÙŠØ¨ 1: ØªØ­Ù„ÙŠÙ„ Ù†Øµ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø¨Ø­Ø«
# ------------------------
with tab1:
    st.subheader("ğŸ” Ø£Ø¯Ø®Ù„ Ù†ØµÙ‹Ø§ Ù„ØªØ­Ù„ÙŠÙ„Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø©:")
    search_text = st.text_input("âœï¸ Ø§ÙƒØªØ¨ Ø¬Ù…Ù„Ø© Ø£Ùˆ ØªØ¹Ù„ÙŠÙ‚:")
    if search_text:
        seq = tokenizer.texts_to_sequences([search_text])
        padded = pad_sequences(seq, maxlen=10)
        pred = model.predict(padded, verbose=0)
        label = label_map[np.argmax(pred)]

        st.markdown(f"""
        ---
        ğŸ“ **Ø§Ù„Ù†Øµ:** {search_text}  
        ğŸ§  **Ø§Ù„ØªÙˆØ¬Ù‡:** `{label}`
        """)

# ------------------------
# Ø§Ù„ØªØ¨ÙˆÙŠØ¨ 2: ØªØ­Ù„ÙŠÙ„ ØªØºØ±ÙŠØ¯Ø§Øª Twitter
# ------------------------
with tab2:
    st.subheader("ğŸ¦ Ø£Ø¯Ø®Ù„ ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ©:")
    keyword = st.text_input("ğŸ”‘ Ù…Ø«Ø§Ù„: Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")
    tweet_count = st.slider("ğŸ”¢ Ø¹Ø¯Ø¯ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª", 5, 50, 10)

    if st.button("ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª"):
        if not keyword:
            st.warning("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ ÙƒÙ„Ù…Ø©.")
        else:
            st.info("ğŸ“¡ Ø¬Ø§Ø±Ù Ø¬Ù„Ø¨ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª...")
            tweets = []
            for i, tweet in enumerate(sntwitter.TwitterSearchScraper(keyword).get_items()):
                if i >= tweet_count:
                    break
                tweets.append(tweet.content)

            for tweet in tweets:
                seq = tokenizer.texts_to_sequences([tweet])
                padded = pad_sequences(seq, maxlen=10)
                pred = model.predict(padded, verbose=0)
                label = label_map[np.argmax(pred)]
                st.markdown(f"""
                ---
                ğŸ“ **Ø§Ù„Ù†Øµ:** {tweet}  
                ğŸ§  **Ø§Ù„ØªÙˆØ¬Ù‡:** `{label}`
                """)

# ------------------------
# Ø§Ù„ØªØ¨ÙˆÙŠØ¨ 3: Ø¨Ø­Ø« ÙˆØªØ­Ù„ÙŠÙ„ Ù…Ù†Ø´ÙˆØ±Ø§Øª ÙˆÙ‡Ù…ÙŠØ©
# ------------------------
with tab3:
    st.subheader("ğŸ“‚ Ù…Ù†Ø´ÙˆØ±Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©")

    posts = [
        "I love this product!",
        "Worst experience ever.",
        "Totally fine and average.",
        "Amazing work from the team.",
        "Horrible service.",
        "Not good, not bad."
    ]

    query = st.text_input("ğŸ” Ø§Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª:")
    if query:
        results = [p for p in posts if query.lower() in p.lower()]
        if results:
            st.write("### âœ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
            for res in results:
                st.write(f"- {res}")
                seq = tokenizer.texts_to_sequences([res])
                padded = pad_sequences(seq, maxlen=10)
                pred = model.predict(padded, verbose=0)
                label = label_map[np.argmax(pred)]
                st.write(f"ğŸ§  Ø§Ù„ØªÙˆØ¬Ù‡: `{label}`")
        else:
            st.warning("ğŸš« Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…Ø·Ø§Ø¨Ù‚Ø©.")
